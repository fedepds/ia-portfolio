# Pr谩ctico 5 - Validaci贸n y Selecci贸n de Modelos

##  Objetivo
El prop贸sito de este pr谩ctico fue aprender a:
1. Prevenir **data leakage** utilizando pipelines.
2. Implementar **validaci贸n cruzada** (cross-validation) robusta.
3. Comparar m煤ltiples modelos de forma sistem谩tica.
4. Analizar m茅tricas de estabilidad y seleccionar el modelo m谩s adecuado.

---

##  Contexto y Dataset
- **Problema**: predecir **abandono estudiantil** y **茅xito acad茅mico** en educaci贸n superior.  
- **Objetivo aplicado**: identificar estudiantes en riesgo para implementar estrategias de apoyo.  
- **Variables**: 36 caracter铆sticas (demogr谩ficas, acad茅micas, socioecon贸micas).  
- **Valor**: mejorar la **retenci贸n estudiantil** y reducir la tasa de abandono.  

---

##  Proceso

### 1. Preparaci贸n de datos
- Se construyeron **pipelines** para estandarizar el preprocesamiento y evitar fugas de datos.  
- Se aplic贸 divisi贸n estratificada de los datos para mantener balance entre clases.

### 2. Validaci贸n cruzada
- Se utiliz贸 **K-Fold Cross Validation** para obtener estimaciones m谩s confiables del rendimiento de los modelos.  
- Esta t茅cnica permiti贸 observar no solo la media de la m茅trica, sino tambi茅n la **variabilidad entre folds**.

### 3. Modelos evaluados
- **DummyClassifier**  
  - Modelo base que predice siempre la clase m谩s frecuente.  
  - Sirve como referencia m铆nima: cualquier modelo 煤til debe superarlo.  

- **Regresi贸n Log铆stica**  
  - Modelo lineal para clasificaci贸n.  
  - Estima la probabilidad de pertenecer a una clase mediante la funci贸n log铆stica.  
  - Sencillo, interpretable y suele ser muy robusto.  

- **Random Forest**  
  - Conjunto de m煤ltiples 谩rboles de decisi贸n entrenados en subconjuntos de datos.  
  - Reduce la varianza de un 谩rbol individual y mejora la generalizaci贸n.  

- **Gradient Boosting (ej. XGBoost/LightGBM)**  
  - Algoritmo de ensamble que construye 谩rboles de forma secuencial, cada uno corrigiendo los errores del anterior.  
  - Tiende a ser muy preciso, aunque m谩s sensible a par谩metros y con mayor riesgo de sobreajuste.  

---

##  Resultados

- El **DummyClassifier** sirvi贸 como referencia m铆nima.  
- La **Regresi贸n Log铆stica** mostr贸 un rendimiento s贸lido y estable.  
- **Random Forest** y **Gradient Boosting** presentaron buena performance, pero con mayor varianza en algunos folds.  
- Seg煤n los boxplots de validaci贸n cruzada, la **Logistic Regression fue la m谩s estable**, mientras que las diferencias en bigotes de otros modelos pudieron deberse a outliers o folds at铆picos.

---

##  An谩lisis
- La **estabilidad del modelo** es tan importante como la m茅trica promedio: un modelo con menor varianza ofrece predicciones m谩s confiables.  
- En este caso, la Regresi贸n Log铆stica result贸 competitiva y consistente, lo que la hace un buen candidato para producci贸n.  

---

##  Reflexi贸n y mejoras
- Se recomienda probar **nuevas features derivadas** (interacciones, combinaciones de variables socioecon贸micas y acad茅micas).  
- Explorar m茅todos de **regularizaci贸n** (Ridge, Lasso) en la regresi贸n log铆stica.  
- Implementar t茅cnicas de **balanceo de clases** (SMOTE, undersampling) si la variable objetivo est谩 desbalanceada.  
- Comparar con modelos m谩s complejos como **Redes Neuronales** o **Stacking** para ensambles.  

---
