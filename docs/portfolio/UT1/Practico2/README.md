# PrÃ¡ctico 2 - Feature Engineering y Modelo Base

## ğŸ¯ Objetivo
Ir mÃ¡s allÃ¡ del anÃ¡lisis exploratorio inicial del dataset Titanic para:
1. Manejar valores faltantes con imputaciÃ³n inteligente.
2. Crear nuevas variables (feature engineering) que capturen informaciÃ³n relevante.
3. Entrenar un **modelo base** con regresiÃ³n logÃ­stica y compararlo con un **baseline** (DummyClassifier).

---

## ğŸ“Š Dataset
- **Fuente**: [Titanic - Kaggle](https://www.kaggle.com/c/titanic)
- **Variable Importante**: `Survived` (0 = no sobreviviÃ³, 1 = sobreviviÃ³).

---

## ğŸ”§ Proceso

### 1. Preprocesamiento
- **Embarked** â†’ valores faltantes imputados con la moda, por ser el mas comÃºn
- **Fare** â†’ valores faltantes imputados con la mediana para evitar que valores extremos la distorsionen
- **Age** â†’ l.                 a mediana de la edad se calculÃ³ para cada grupo de Sex y Pclass, lo que resulta en una imputaciÃ³n mÃ¡s precisa.

### 2. Feature Engineering 
- Es crear nuevas variables a partir de las existentes para mejorar el rendimiento del modelo.
- `FamilySize` = `SibSp` + `Parch` + 1  
- `IsAlone` = 1 si el pasajero viajaba solo, 0 en caso contrario 
- FamilySize y IsAlone: Se crearon para capturar el tamaÃ±o de la familia de un pasajero, combinando SibSp y Parch. La intuiciÃ³n es que las familias podrÃ­an haber tenido una ventaja o desventaja en la supervivencia.
- `Title` = extracciÃ³n del tÃ­tulo desde la columna `Name` (Mr., Mrs., Miss., Rare, etc.), para poder agrupar a las personas quitando el estatus social.  

### 3. TransformaciÃ³n
- **one-hot encoding** (`pd.get_dummies`). Para convertir las variables categÃ³ricas (Sex, Embarked, Title) en variables numÃ©ricas binarias que el modelo pueda procesar.


### 4. Modelos evaluados
- **DummyClassifier** â†’ baseline (predice siempre la clase mÃ¡s frecuente).Su precisiÃ³n nos dio el rendimiento mÃ­nimo que cualquier modelo Ãºtil deberÃ­a superar.
- **LogisticRegression** â†’ Este modelo resuelve problemas de clasificaciÃ³n.
- **train_test_split:** â†’ FunciÃ³n para dividir los datos de entrenamiento y de prueba

---

## ğŸ“ˆ Resultados

- **Baseline (DummyClassifier)**: Acc â‰ˆ *0.62*.  
- **RegresiÃ³n LogÃ­stica**: Acc â‰ˆ *0.79*.  

ğŸ“Œ El modelo de regresiÃ³n logÃ­stica **supera claramente al baseline**, lo que confirma que las features creadas y el preprocesamiento aportan informaciÃ³n valiosa.

---

## ğŸ” AnÃ¡lisis de la matriz de confusiÃ³n
- **Falsos positivos**: casos donde el modelo predijo supervivencia pero en realidad no ocurriÃ³.  
- **Falsos negativos**: casos donde el modelo predijo no supervivencia pero la persona sÃ­ sobreviviÃ³.  
ğŸ‘‰ En este contexto, los **falsos negativos son mÃ¡s graves**, porque implican â€œno salvarâ€ a alguien que sÃ­ podÃ­a sobrevivir.  

El modelo tiende a equivocarse mÃ¡s con los **no sobrevivientes**.

---

## ğŸš€ ReflexiÃ³n y mejoras
- El **feature engineering** aportÃ³ mucho valor: especialmente `Title` y `FamilySize`.  
- A futuro, se podrÃ­an crear nuevas variables a partir de:
  - La cabina (`Cabin`) â†’ ubicaciÃ³n en el barco.
  - El billete (`Ticket`) â†’ posibles grupos de viaje.  

Esto abrirÃ­a la puerta a modelos mÃ¡s complejos como **Ã¡rboles de decisiÃ³n o random forest**.

---
